# üìä Job Market Data Engineering Pipeline (PostgreSQL + Python)

## üìå Overview

This project is a **data engineering pipeline** that automates the execution of analytical SQL queries on a PostgreSQL database using Python.

The pipeline is designed to:

- Connect to a relational database  
- Execute multiple SQL files programmatically  
- Orchestrate queries in a reproducible way  
- Export results into structured formats for downstream analysis  

This project focuses on **engineering, orchestration, and automation**, rather than pure analysis.

---

## üéØ Project Objectives

- Build a reusable **Python‚ÄìPostgreSQL integration**
- Automate execution of multiple SQL analysis queries
- Separate configuration, connection logic, and execution logic
- Demonstrate real-world **data engineering best practices**
- Produce clean, reproducible outputs from raw database tables

## üèóÔ∏è Architecture Overview

PostgreSQL Database
‚Üì
SQL files (.sql)
‚Üì
Python Orchestration Layer
‚Üì
Structured Outputs (CSV)


### Key Design Principles

- Separation of concerns  
- Reusable database connections  
- Configurable and scalable pipeline  
- Safe handling of large datasets (not committed to Git)

---

## üõ†Ô∏è Tech Stack

| Component        | Technology |
|------------------|------------|
| Database         | PostgreSQL |
| Language         | Python 3 |
| SQL              | Advanced SQL (CTEs, joins, aggregations) |
| DB Connector     | psycopg2 |
| Version Control  | Git & GitHub |
| Environment      | Python virtual environment (venv) |

---

## üìÇ Project Structure

```text
job-market-data-pipeline/
‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îú‚îÄ‚îÄ db_config.py           # Database configuration (local only)
‚îÇ   ‚îú‚îÄ‚îÄ db_connection.py       # Reusable PostgreSQL connection logic
‚îÇ   ‚îú‚îÄ‚îÄ run_queries.py         # Executes SQL files dynamically
‚îÇ   ‚îú‚îÄ‚îÄ export_to_csv.py       # Exports query results to CSV
‚îÇ   ‚îú‚îÄ‚îÄ test_connection.py     # Connection sanity check
‚îÇ   ‚îî‚îÄ‚îÄ analyze_results.py     # Reserved for future transformations
‚îÇ
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore
```

---

## ‚öôÔ∏è Pipeline Workflow

### 1Ô∏è‚É£ Database Connection
- PostgreSQL credentials are defined locally  
- Python establishes a secure connection using `psycopg2`

### 2Ô∏è‚É£ SQL Orchestration
- SQL queries are stored as standalone `.sql` files  
- Python dynamically reads and executes them  
- This allows easy query updates without changing Python logic  

### 3Ô∏è‚É£ Query Execution
- Queries are executed sequentially  
- Results are fetched with schema metadata (column names)

### 4Ô∏è‚É£ Output Generation
- Results are exported as CSV files (locally)  
- Large datasets are intentionally excluded from GitHub  

---

## üß™ Scripts Explained

### `db_connection.py`
- Centralized database connection handler  
- Prevents duplicated connection logic  
- Implements safe connection opening and closing

```bash
import psycopg2
from db_config import DB_CONFIG

def get_connection():
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        print("‚úÖ Connected to PostgreSQL (sql_course)")
        return conn
    except Exception as e:
        print("‚ùå Failed to connect to PostgreSQL")
        print(e)
        return None

``` 

------

### `run_queries.py`
- Reads all SQL files from a directory  
- Executes them programmatically  
- Prints row counts and sample results  
- Acts as the orchestration layer

```bash
import os
from db_connection import get_connection

SQL_FOLDER = "../project_sql"

def read_sql_file(filepath):
    with open(filepath, "r") as file:
        return file.read()

def run_query(sql_query):
    conn = get_connection()
    if not conn:
        return None

    try:
        cur = conn.cursor()
        cur.execute(sql_query)
        rows = cur.fetchall()
        return rows
    except Exception as e:
        print("‚ùå Query failed:")
        print(e)
        return None
    finally:
        conn.close()

def main():
    sql_files = sorted([
        f for f in os.listdir(SQL_FOLDER)
        if f.endswith(".sql")
    ])

    for file in sql_files:
        print("\n" + "="*60)
        print(f"‚ñ∂ Running query: {file}")
        print("="*60)

        sql_path = os.path.join(SQL_FOLDER, file)
        query = read_sql_file(sql_path)
        results = run_query(query)

        if results:
            print(f"‚úÖ Rows returned: {len(results)}")
            print("üîπ Sample rows:")
            for row in results[:5]:
                print(row)
        else:
            print("‚ö†Ô∏è No results or query failed")

if __name__ == "__main__":
    main()

```

-----

### `export_to_csv.py`
- Extends query execution  
- Extracts column names automatically  
- Saves structured CSV outputs for analysis

```bash
import os
import csv
from db_connection import get_connection

SQL_FOLDER = "../project_sql"
OUTPUT_FOLDER = "../csv_files"

os.makedirs(OUTPUT_FOLDER, exist_ok=True)

def read_sql_file(filepath):
    with open(filepath, "r") as file:
        return file.read()

def run_query_with_columns(sql_query):
    conn = get_connection()
    if not conn:
        return None, None

    try:
        cur = conn.cursor()
        cur.execute(sql_query)
        rows = cur.fetchall()
        columns = [desc[0] for desc in cur.description]
        return columns, rows
    except Exception as e:
        print("‚ùå Query failed:")
        print(e)
        return None, None
    finally:
        conn.close()

def export_csv(filename, columns, rows):
    path = os.path.join(OUTPUT_FOLDER, filename)

    with open(path, "w", newline="") as file:
        writer = csv.writer(file)
        writer.writerow(columns)
        writer.writerows(rows)

    print(f"üìÅ Exported: {path}")

def main():
    sql_files = sorted([
        f for f in os.listdir(SQL_FOLDER)
        if f.endswith(".sql")
    ])

    for file in sql_files:
        print(f"\n‚ñ∂ Processing {file}")
        sql_path = os.path.join(SQL_FOLDER, file)
        query = read_sql_file(sql_path)

        columns, rows = run_query_with_columns(query)

        if rows:
            csv_name = file.replace(".sql", ".csv")
            export_csv(csv_name, columns, rows)
        else:
            print("‚ö†Ô∏è No data to export")

if __name__ == "__main__":
    main()

```

------

### `test_connection.py`
- Lightweight sanity check  
- Confirms PostgreSQL connectivity before running the pipeline  

```bash
from db_connection import get_connection

print("üöÄ Starting connection test")

conn = get_connection()

if conn:
    conn.close()
    print("üîå Connection closed successfully")

```

------

### `analyze_results.py`
- Placeholder for future transformations  
- Intended for:
  - Pandas analysis  
  - Feature engineering  
  - Business metrics computation
 
```bash
"""
analyze_results.py

This module is reserved for future data analysis and transformations.
Possible extensions:
- Aggregations using pandas
- Business KPIs computation
- Feature engineering for downstream analytics or ML
"""

```

---

## üîê Data & Security Handling

- Large datasets are **not committed** to GitHub  
- Raw tables are stored locally or in the database  
- Sensitive credentials are excluded via `.gitignore`  
- This follows real-world data engineering standards  

---

## üöÄ How to Run the Project Locally

### 1Ô∏è‚É£ Create & Activate Virtual Environment
```bash
python3 -m venv venv
source venv/bin/activate
```

### 2Ô∏è‚É£ Install Dependencies
```bash
pip install psycopg2-binary
```
### 3Ô∏è‚É£ Test Database Connection
```bash
python python/test_connection.py
```
### 4Ô∏è‚É£ Execute SQL Queries
```bash
python python/run_queries.py
```
### 5Ô∏è‚É£ Export Results to CSV
```bash
python python/export_to_csv.py
```

## üìà What This Project Demonstrates

- Practical data engineering mindset  
- Python‚ÄìPostgreSQL integration  
- Query orchestration and automation  
- Clean project structure and modular design  
- Safe handling of large datasets  
- Production-ready Git workflow  

---

## üíº Use Case

This pipeline can be adapted for:

- Analytics engineering  
- ETL pipelines  
- Automated reporting  
- Data warehouse query orchestration  
- Preprocessing layers for machine learning  

---

## üß≠ Future Improvements

- Environment variables for configuration  
- Pandas-based transformation layer  
- Logging instead of print statements  
- Scheduling with Airflow or cron  
- Dockerization for deployment  



